import { cleanPromptString } from "./util.js";

export const openrouter_meta_llama_3_2_3b_instruct_free = {
  id: "openrouter-meta-llama-3.2-3b-instruct-free",
  name: "Meta Llama 3.2 Free",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [
    { key: "Content-Type", value: "application/json" },
    { key: "HTTP-Referer", value: "https://www.flowrabbit.ai" },
    { key: "X-Title", value: "Flowrabbit" },
  ],
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "prompt",
      label: "Prompt",
      placeholder: "Write your prompt",
      class: "MatcAutoCompleteTextareaXS",
    },
    {
      type: "Number",
      id: "max_tokens",
      label: "Max Tokens",
      default: 4096,
      helper: "Maximum number of tokens to generate (Limit: 4096).",
      required: true,
      min: 1,
      max: 4096,
    },
  ],
  advanced: [
    {
      default: 0.7,
      min: 0,
      max: 1,
      type: "range",
      required: true,
      id: "temperature",
      label: "Temperature",
      helper: "Controls randomness. 1.0 is the most random, 0.0 is deterministic.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      type: "range",
      id: "top_p",
      label: "Top P",
      helper: "Limits token selection to a cumulative probability threshold.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      type: "range",
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Discourages using tokens from the prompt.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0,
      max: 2.0,
      type: "range",
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Discourages frequent token repetition.",
      decimals: true,
    },
  ],
  method: "POST",
  authType: "Bearer",
  documentationAuthLink: "https://replicate.com/docs/reference/http#authentication",
  getTemplate: (vars) => {
    return {
      model: "mistralai/mistral-7b-instruct:free",
      messages: [
        {
          role: "user",
          content: cleanPromptString(vars.prompt),
        },
      ],
      temperature: vars.temperature,
      max_tokens: vars.max_tokens,
      top_p: vars.top_p,
      presence_penalty: vars.presence_penalty,
      frequency_penalty: vars.frequency_penalty,
      stream: vars.stream,
    };
  },
  getMaxTokens: (vars) => vars.max_tokens,
  output: {
    path: "choices[0].message.content",
    type: "JSON",
    streampath: "choices[0].delta.content",
  },
};

export const openrouter_pixtral_12b_free = {
  id: "openrouter-pixtral-12b-free",
  name: "MistralAI Pixtral 12B Free",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [{ key: "Content-Type", value: "application/json" }],
  authType: "Bearer",
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "prompt",
      label: "Prompt",
      placeholder: "Enter your prompt",
    },
    {
      type: "Number",
      id: "max_tokens",
      label: "Max Tokens",
      default: 4096,
      helper: "Maximum number of tokens to generate (Limit: 4096).",
      required: true,
      min: 1,
      max: 4096,
    },
  ],
  advanced: [
    {
      default: 0.7,
      min: 0,
      max: 1,
      type: "range",
      required: true,
      id: "temperature",
      label: "Temperature",
      helper: "Adjusts randomness in output. 1.0 is most random, 0.0 is deterministic.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      type: "range",
      id: "top_p",
      label: "Top P",
      helper: "Restricts token selection to a cumulative probability threshold.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      type: "range",
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Discourages repeating tokens from the prompt.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0,
      max: 2.0,
      type: "range",
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Discourages repeating frequently used tokens.",
      decimals: true,
    },
  ],
  method: "POST",
  output: {
    path: "choices[0].message.content",
    type: "JSON",
  },
  getTemplate: (vars) => {
    return {
      model: "mistralai/pixtral-12b:free",
      messages: [{ role: "user", content: vars.prompt }],
      temperature: vars.temperature,
      max_tokens: vars.max_tokens,
      top_p: vars.top_p,
      presence_penalty: vars.presence_penalty,
      frequency_penalty: vars.frequency_penalty,
      stream: vars.stream, // Added stream property for LLMs
    };
  },
  getMaxTokens: (vars) => vars.max_tokens, // Added getMaxTokens function
};

export const openrouter_gemini_flash = {
  id: "openrouter-gemini-flash",
  name: "Gemini Flash",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [
    { key: "Content-Type", value: "application/json" },
    { key: "HTTP-Referer", value: "https://www.flowrabbit.ai" },
    { key: "X-Title", value: "Flowrabbit" },
  ],
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "prompt",
      label: "Prompt",
      placeholder: "Write your prompt",
    },
    {
      type: "Number",
      id: "max_tokens",
      label: "Max Tokens",
      default: 2048,
      helper: "Maximum tokens to generate. The limit is 4096.",
      required: true,
      min: 1,
      max: 4096,
    },
  ],
  advanced: [
    {
      default: 0.7,
      min: 0,
      max: 1,
      type: "range",
      decimals: true,
      required: true,
      id: "temperature",
      label: "Temperature",
      helper: "Controls randomness. 1.0 is the most random, 0.0 is the most deterministic.",
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      type: "range",
      decimals: true,
      id: "top_p",
      label: "Top P",
      helper: "Limits token selection to a cumulative probability threshold.",
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      type: "range",
      decimals: true,
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Discourages reusing tokens from the prompt.",
    },
    {
      default: 0.0,
      min: 0,
      max: 2.0,
      type: "range",
      decimals: true,
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Discourages repeating tokens.",
    },
  ],
  method: "POST",
  authType: "Bearer",
  documentationAuthLink: "https://replicate.com/docs/reference/http#authentication",
  getTemplate: (vars) => {
    return {
      model: "google/gemini-flash-1.5",
      messages: [
        { role: "user", content: cleanPromptString(vars.prompt) },
        {
          role: "system",
          content: cleanPromptString(vars.systemprompt),
        },
      ],
      stream: vars.stream,
      temperature: vars.temperature,
      max_tokens: vars.max_tokens,
      top_p: vars.top_p,
      presence_penalty: vars.presence_penalty,
      frequency_penalty: vars.frequency_penalty,
    };
  },
  getMaxTokens: (vars) => vars.max_tokens,
  output: {
    path: "choices[0].message.content",
    type: "JSON",
    streampath: "choices[0].delta.content",
  },
};

export const openrouter_mythomax_l2_13b = {
  id: "openrouter-mythomax-l2-13b",
  name: "Gryphe Mythomax L2 13B",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [{ key: "Content-Type", value: "application/json" }],
  authType: "Bearer",
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "prompt",
      label: "Prompt",
      placeholder: "Enter your prompt",
    },
    {
      type: "Number",
      id: "max_tokens",
      label: "Max Tokens",
      default: 4096,
      helper: "Maximum number of tokens to generate. Limit: 4096.",
      required: true,
      min: 1,
      max: 4096,
    },
  ],
  advanced: [
    {
      default: 0.7,
      min: 0,
      max: 1,
      type: "range",
      required: true,
      decimals: true,
      id: "temperature",
      label: "Temperature",
      helper: "Controls randomness (0.0: deterministic, 1.0: most random).",
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      decimals: true,
      type: "range",
      id: "top_p",
      label: "Top P",
      helper: "Token selection limited to a cumulative probability threshold.",
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      decimals: true,
      type: "range",
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Discourages repeated tokens from the prompt.",
    },
    {
      default: 0.0,
      min: 0,
      max: 2.0,
      decimals: true,
      type: "range",
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Discourages frequent token repetition.",
    },
  ],
  method: "POST",
  output: {
    path: "choices[0].message.content",
    type: "JSON",
  },
  getTemplate: (vars) => {
    return {
      model: "gryphe/mythomax-l2-13b",
      messages: [{ role: "user", content: vars.prompt }],
      temperature: vars.temperature,
      max_tokens: vars.max_tokens,
      top_p: vars.top_p,
      presence_penalty: vars.presence_penalty,
      frequency_penalty: vars.frequency_penalty,
      stream: vars.stream,
    };
  },
  getMaxTokens: (vars) => {
    return vars.max_tokens;
  },
};

export const openrouter_mythomax_l2_13b_free = {
  id: "openrouter-mythomax-l2-13b-free",
  name: "Gryphe Mythomax L2 13B Free",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [{ key: "Content-Type", value: "application/json" }],
  authType: "Bearer",
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "prompt",
      label: "Prompt",
      placeholder: "Enter your prompt",
    },
    {
      type: "Number",
      id: "max_tokens",
      label: "Max Tokens",
      default: 4096,
      helper: "Maximum number of tokens to generate. Limit: 4096.",
      required: true,
      min: 1,
      max: 4096,
    },
  ],
  advanced: [
    {
      default: 0.7,
      min: 0,
      max: 1,
      type: "range",
      required: true,
      decimals: true,
      id: "temperature",
      label: "Temperature",
      helper: "Controls randomness (0.0: deterministic, 1.0: most random).",
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      decimals: true,
      type: "range",
      id: "top_p",
      label: "Top P",
      helper: "Token selection limited to a cumulative probability threshold.",
    },
    {
      default: 1.0,
      min: 0,
      max: 2.0,
      decimals: true,
      type: "range",
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Discourages repeated tokens from the prompt.",
    },
    {
      default: 0.0,
      min: 0,
      max: 2.0,
      decimals: true,
      type: "range",
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Discourages frequent token repetition.",
    },
  ],
  method: "POST",
  output: {
    path: "choices[0].message.content",
    type: "JSON",
  },
  getTemplate: (vars) => {
    return {
      model: "gryphe/mythomax-l2-13b:free",
      messages: [{ role: "user", content: vars.prompt }],
      temperature: vars.temperature,
      max_tokens: vars.max_tokens,
      top_p: vars.top_p,
      presence_penalty: vars.presence_penalty,
      frequency_penalty: vars.frequency_penalty,
      stream: vars.stream,
    };
  },
  getMaxTokens: (vars) => {
    return vars.max_tokens;
  },
};

export const openrouter_llama = {
  id: "llm-openrouter-llama",
  name: "Llama 3.1 70B",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [
    { key: "Content-Type", value: "application/json" },
    { key: "HTTP-Referer", value: "https://www.flowrabbit.ai" },
    { key: "X-Title", value: "Flowrabbit" },
  ],
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "user_message",
      label: "User Message",
      placeholder: "What is the meaning of life?",
      helper: "Enter the message for the model to respond to",
    },
  ],
  method: "POST",
  authType: "Bearer",
  documentationAuthLink: "https://openrouter.ai/docs",
  getTemplate: (vars) => ({
    model: "meta-llama/llama-3.1-70b-instruct",
    messages: [{ role: "user", content: vars.user_message }],
    stream: vars.stream,
  }),
  output: {
    path: "choices[0].message.content",
    type: "JSON",
    streampath: "choices[0].delta.content",
  },
};

export const openrouter_antrophic_claude = {
  id: "llm-openrouter-antrophic-claude",
  name: "Claude 3.5 Sonnet",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [
    { key: "Content-Type", value: "application/json" },
    { key: "HTTP-Referer", value: "https://www.flowrabbit.ai" },
    { key: "X-Title", value: "Flowrabbit" },
  ],
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "user_message",
      label: "User Message",
      placeholder: "What is the meaning of life?",
      helper: "Enter the message for the model to respond to",
    },
  ],
  advanced: [
    {
      default: 1.0,
      min: 0.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "temperature",
      label: "Temperature",
      helper: "Controls the variety in responses.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "top_p",
      label: "Top P",
      helper: "Limits the model's choices to a percentage of likely tokens.",
      decimals: true,
    },
    {
      default: 0,
      min: 0,
      type: "Number",
      required: false,
      id: "top_k",
      label: "Top K",
      helper: "Limits the model's choice of tokens at each step.",
    },
    {
      default: 0.0,
      min: -2.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Controls the repetition of tokens.",
      decimals: true,
    },
    {
      default: 0.0,
      min: -2.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Adjusts the likelihood of repeating tokens.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "repetition_penalty",
      label: "Repetition Penalty",
      helper: "Reduces the repetition of tokens from the input.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "min_p",
      label: "Min P",
      helper: "Minimum probability for a token to be considered.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "top_a",
      label: "Top A",
      helper: "Focuses choices based on the highest probability token.",
      decimals: true,
    },
  ],
  method: "POST",
  authType: "Bearer",
  documentationAuthLink: "https://openrouter.ai/docs",
  getTemplate: (vars) => ({
    model: "anthropic/claude-3.5-sonnet",
    messages: [{ role: "user", content: vars.user_message }],
    temperature: vars.temperature,
    top_p: vars.top_p,
    top_k: vars.top_k,
    frequency_penalty: vars.frequency_penalty,
    presence_penalty: vars.presence_penalty,
    repetition_penalty: vars.repetition_penalty,
    min_p: vars.min_p,
    top_a: vars.top_a,
    stream: vars.stream,
  }),
  output: {
    path: "choices[0].message.content",
    type: "JSON",
    streampath: "choices[0].delta.content",
  },
};

export const openrouter_gemini_pro = {
  id: "llm-openrouter-gemini-pro",
  name: "Gemini Pro",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [
    { key: "Content-Type", value: "application/json" },
    { key: "HTTP-Referer", value: "https://www.flowrabbit.ai" },
    { key: "X-Title", value: "Flowrabbit" },
  ],
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "user_message",
      label: "User Message",
      placeholder: "What is the meaning of life?",
      helper: "Enter the message for the model to respond to",
    },
  ],
  advanced: [
    {
      default: 1.0,
      min: 0.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "temperature",
      label: "Temperature",
      helper: "Controls the variety in responses.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "top_p",
      label: "Top P",
      helper: "Limits the model's choices to a percentage of likely tokens.",
      decimals: true,
    },
    {
      default: 0,
      min: 0,
      type: "Number",
      required: false,
      id: "top_k",
      label: "Top K",
      helper: "Limits the model's choice of tokens at each step.",
    },
    {
      default: 0.0,
      min: -2.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Controls the repetition of tokens.",
      decimals: true,
    },
    {
      default: 0.0,
      min: -2.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Adjusts the likelihood of repeating tokens.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "repetition_penalty",
      label: "Repetition Penalty",
      helper: "Reduces the repetition of tokens from the input.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "min_p",
      label: "Min P",
      helper: "Minimum probability for a token to be considered.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "top_a",
      label: "Top A",
      helper: "Focuses choices based on the highest probability token.",
      decimals: true,
    },
  ],
  method: "POST",
  authType: "Bearer",
  documentationAuthLink: "https://openrouter.ai/docs",
  getTemplate: (vars) => ({
    model: "google/gemini-pro-1.5-exp",
    messages: [{ role: "user", content: vars.user_message }],
    temperature: vars.temperature,
    top_p: vars.top_p,
    top_k: vars.top_k,
    frequency_penalty: vars.frequency_penalty,
    presence_penalty: vars.presence_penalty,
    repetition_penalty: vars.repetition_penalty,
    min_p: vars.min_p,
    top_a: vars.top_a,
    stream: vars.stream,
  }),
  output: {
    path: "choices[0].message.content",
    type: "JSON",
    streampath: "choices[0].delta.content",
  },
};

export const openrouter_gpt_mini = {
  id: "llm-openrouter-gpt-mini",
  name: "GPT Mini",
  url: "https://openrouter.ai/api/v1/chat/completions",
  type: "openrouter",
  headers: [
    { key: "Content-Type", value: "application/json" },
    { key: "HTTP-Referer", value: "https://www.flowrabbit.ai" },
    { key: "X-Title", value: "Flowrabbit" },
  ],
  elements: [
    {
      type: "TextArea",
      required: true,
      id: "user_message",
      label: "User Message",
      placeholder: "What is the meaning of life?",
      helper: "Enter the message for the model to respond to",
    },
  ],
  advanced: [
    {
      default: 1.0,
      min: 0.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "temperature",
      label: "Temperature",
      helper: "Controls the variety in responses.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "top_p",
      label: "Top P",
      helper: "Limits the model's choices to a percentage of likely tokens.",
      decimals: true,
    },
    {
      default: 0,
      min: 0,
      type: "Number",
      required: false,
      id: "top_k",
      label: "Top K",
      helper: "Limits the model's choice of tokens at each step.",
    },
    {
      default: 0.0,
      min: -2.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "frequency_penalty",
      label: "Frequency Penalty",
      helper: "Controls the repetition of tokens.",
      decimals: true,
    },
    {
      default: 0.0,
      min: -2.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "presence_penalty",
      label: "Presence Penalty",
      helper: "Adjusts the likelihood of repeating tokens.",
      decimals: true,
    },
    {
      default: 1.0,
      min: 0.0,
      max: 2.0,
      type: "range",
      required: false,
      id: "repetition_penalty",
      label: "Repetition Penalty",
      helper: "Reduces the repetition of tokens from the input.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "min_p",
      label: "Min P",
      helper: "Minimum probability for a token to be considered.",
      decimals: true,
    },
    {
      default: 0.0,
      min: 0.0,
      max: 1.0,
      type: "range",
      required: false,
      id: "top_a",
      label: "Top A",
      helper: "Focuses choices based on the highest probability token.",
      decimals: true,
    },
  ],
  method: "POST",
  authType: "Bearer",
  documentationAuthLink: "https://openrouter.ai/docs",
  getTemplate: (vars) => ({
    model: "openai/gpt-4o-mini",
    messages: [{ role: "user", content: vars.user_message }],
    temperature: vars.temperature,
    top_p: vars.top_p,
    top_k: vars.top_k,
    frequency_penalty: vars.frequency_penalty,
    presence_penalty: vars.presence_penalty,
    repetition_penalty: vars.repetition_penalty,
    min_p: vars.min_p,
    top_a: vars.top_a,
    stream: vars.stream,
  }),
  output: {
    path: "choices[0].message.content",
    type: "JSON",
    streampath: "choices[0].delta.content",
  },
};
